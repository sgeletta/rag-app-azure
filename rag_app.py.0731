import os
import csv
import hashlib
import sqlite3
import streamlit as st
import pandas as pd
from datetime import datetime

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader,
    UnstructuredMarkdownLoader, CSVLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA

# ---- Setup ----
LOG_DIR = "logs"
INDEX_DIR = "indexes"
DB_FILE = os.path.join(LOG_DIR, "query_log.db")
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(INDEX_DIR, exist_ok=True)

def load_document(file_path):
    ext = os.path.splitext(file_path)[-1].lower()
    if ext == ".txt":
        return TextLoader(file_path).load()
    elif ext == ".pdf":
        return PyPDFLoader(file_path).load()
    elif ext == ".docx":
        return Docx2txtLoader(file_path).load()
    elif ext == ".csv":
        return CSVLoader(file_path).load()
    elif ext == ".md":
        return UnstructuredMarkdownLoader(file_path).load()
    else:
        raise ValueError("Unsupported file type")

def log_to_db(question, answer, doc_list):
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT,
            documents TEXT,
            question TEXT,
            answer TEXT
        )
    """)
    cur.execute(
        "INSERT INTO logs (timestamp, documents, question, answer) VALUES (?, ?, ?, ?)",
        (datetime.now().isoformat(), ", ".join(doc_list), question, answer)
    )
    conn.commit()
    conn.close()

def main():
    st.set_page_config(page_title="Local RAG Q&A", layout="wide")
    st.title("üìö Local RAG App (Multi-Doc + SQLite Logging)")

    uploaded_files = st.file_uploader(
        "Upload one or more documents", type=["pdf", "txt", "docx", "csv", "md"], accept_multiple_files=True
    )

    if uploaded_files:
        temp_paths = []
        file_contents = b""
        for uploaded_file in uploaded_files:
            ext = uploaded_file.name.split(".")[-1]
            temp_path = f"temp_{uploaded_file.name}"
            with open(temp_path, "wb") as f:
                data = uploaded_file.getbuffer()
                f.write(data)
                file_contents += data
            temp_paths.append(temp_path)

        fingerprint = hashlib.md5(file_contents).hexdigest()
        faiss_path = os.path.join(INDEX_DIR, f"faiss_{fingerprint}")

        if os.path.exists(faiss_path):
            st.info("üîÅ Loading cached vector index...")
            embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")
            db = FAISS.load_local(faiss_path, embeddings=embedding, allow_dangerous_deserialization=True)
        else:
            st.info("‚öôÔ∏è Creating index from uploaded files...")
            all_docs = []
            for path in temp_paths:
                all_docs.extend(load_document(path))
            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            chunks = splitter.split_documents(all_docs)

            if not chunks:
                st.error("‚ùå No text chunks were extracted from the uploaded documents.")
                return

            embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")
            db = FAISS.from_documents(chunks, embedding)
            db.save_local(faiss_path)


        llm = OllamaLLM(model="mistral")
        qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())

        query = st.text_input("Ask a question about the documents:")

        if query:
            if st.button("üîç Submit Question"):
                with st.spinner("Thinking..."):
                    result = qa.run(query)
                st.markdown("### üí¨ Answer")
                st.write(result)

                log_to_db(query, result, [f.name for f in uploaded_files])
                st.success("‚úÖ Interaction logged.")


    st.sidebar.header("üìä Query Log")
    if os.path.exists(DB_FILE):
        conn = sqlite3.connect(DB_FILE)
        df = pd.read_sql_query("SELECT * FROM logs ORDER BY timestamp DESC LIMIT 10", conn)
        st.sidebar.dataframe(df)

        if st.sidebar.button("‚¨áÔ∏è Download logs as CSV"):
            csv = df.to_csv(index=False).encode("utf-8")
            st.sidebar.download_button("Download CSV", csv, "query_log.csv", "text/csv")

        conn.close()
    else:
        st.sidebar.info("No queries logged yet.")

if __name__ == "__main__":
    main()