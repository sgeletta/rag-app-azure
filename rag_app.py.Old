import os
import csv
import hashlib
import streamlit as st
import pandas as pd
from datetime import datetime

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader,
    UnstructuredMarkdownLoader, CSVLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA

# ---- Setup ----
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "query_log.csv")
INDEX_DIR = "indexes"
os.makedirs(INDEX_DIR, exist_ok=True)

def get_fingerprint(file_path):
    with open(file_path, "rb") as f:
        file_data = f.read()
    return hashlib.md5(file_data).hexdigest()

def log_interaction(question, answer, doc_name):
    timestamp = datetime.now().isoformat()
    with open(LOG_FILE, mode="a", newline='', encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([timestamp, doc_name, question, answer])

@st.cache_resource
def load_vector_store(file_path):
    fingerprint = get_fingerprint(file_path)
    faiss_path = os.path.join(INDEX_DIR, f"faiss_{fingerprint}")

    if os.path.exists(faiss_path):
        st.info("üîÅ Loading cached vector index...")
        embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")
        return FAISS.load_local(faiss_path, embeddings=embedding)

    st.info("üìÑ Index not found. Creating a new one...")
    ext = os.path.splitext(file_path)[-1].lower()
    if ext == ".txt":
        docs = TextLoader(file_path).load()
    elif ext == ".pdf":
        docs = PyPDFLoader(file_path).load()
    elif ext == ".docx":
        docs = Docx2txtLoader(file_path).load()
    elif ext == ".csv":
        docs = CSVLoader(file_path).load()
    elif ext == ".md":
        docs = UnstructuredMarkdownLoader(file_path).load()
    else:
        raise ValueError("Unsupported file type")

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)
    embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")
    vector_store = FAISS.from_documents(chunks, embedding)

    vector_store.save_local(faiss_path)
    return vector_store

# ---- Streamlit UI ----
def main():
    st.set_page_config(page_title="Local RAG Q&A", layout="wide")
    st.title("üìö Local RAG App (Ollama + LangChain + Streamlit)")

    uploaded_file = st.file_uploader("Upload a document", type=["pdf", "txt", "docx", "csv", "md"])

    if uploaded_file:
        # Save uploaded file temporarily
        file_ext = uploaded_file.name.split(".")[-1]
        temp_path = f"temp_upload.{file_ext}"
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.success("‚úÖ Document uploaded.")
        db = load_vector_store(temp_path)
        llm = OllamaLLM(model="mistral")
        qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())

        query = st.text_input("Ask a question about the document:")
        if query:
            with st.spinner("Thinking..."):
                result = qa.run(query)
            st.markdown("### üí¨ Answer")
            st.write(result)

            log_interaction(query, result, uploaded_file.name)
            st.success("‚úÖ Interaction logged.")

    # Optional: Show logs
    st.sidebar.header("üìä Query Log")
    if os.path.exists(LOG_FILE):
        df = pd.read_csv(LOG_FILE, header=None, names=["Timestamp", "Document", "Question", "Answer"])
        st.sidebar.dataframe(df.tail(10))
    else:
        st.sidebar.info("No queries logged yet.")

if __name__ == "__main__":
    main()
